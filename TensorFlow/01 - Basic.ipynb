{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "Tensors are multi-dimensional arrays with a uniform type\n",
    "All tensors are immutable like Python numbers and strings: you can never update the contents of a tensor, only create a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensors\n",
    "a = tf.constant(1)\n",
    "b = tf.constant([2, 3, 4])\n",
    "c = tf.constant([[1, 2],\n",
    "                  [3, 4],\n",
    "                  [5, 6]], dtype=tf.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3 4 5], shape=(3,), dtype=int32)\n",
      "tf.Tensor([2 3], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# basic operations\n",
    "print(a + b)\n",
    "print(b[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ragged  tensors\n",
    "ragged_list = [\n",
    "    [0, 1, 2, 3],\n",
    "    [4, 5],\n",
    "    [6, 7, 8],\n",
    "    [9]\n",
    "]\n",
    "a = tf.ragged.constant(ragged_list)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 0 0 0]\n",
      " [0 0 2 0]\n",
      " [0 0 0 0]], shape=(3, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Sparse tensor\n",
    "a = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]],\n",
    "                                       values= [1, 2],\n",
    "                                       dense_shape=[3, 4])\n",
    "# print(a)\n",
    "print(tf.sparse.to_dense(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "A tf.Variable represents a tensor whose value can be changed by running ops on it  \n",
    "Calling assign does not (usually) allocate a new tensor; instead, the existing tensor's memory is reused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=1>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variables\n",
    "a = tf.constant(1)\n",
    "v = tf.Variable(a)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=3>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.assign(3)\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation\n",
    "By default the tape only watches trainable variables.  \n",
    "You can use tape.watch() specifically specify the variables to watch.\n",
    "The gradients will not be computed if - \n",
    "1. Replaced a variable with a tensor (Not a variable)\n",
    "2. Did calculations outside of TensorFlow (like using numpy)\n",
    "3. Took gradients through an integer or string (dtype is int or string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=10.0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatic Differentiation\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "#     tape.watch(x) # If x is not a trainable variable use this\n",
    "    y = tf.square(x) + 4 * x + 10 \n",
    "\n",
    "dy_dx = tape.gradient(y, x)\n",
    "dy_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs and functions\n",
    "Graphs are data structures that contain a set of tf.Operation objects, which represent units of computation; and tf.Tensor objects, which represent the units of data that flow between operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager time: 10.377188899999965\n",
      "Graph time: 6.047591000000466\n"
     ]
    }
   ],
   "source": [
    "# Create an oveerride model to classify pictures\n",
    "class SequentialModel(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SequentialModel, self).__init__(**kwargs)\n",
    "        self.flatten = tf.keras.layers.Flatten(input_shape=(28, 28))\n",
    "        self.dense_1 = tf.keras.layers.Dense(128, activation=\"relu\")\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.dense_2 = tf.keras.layers.Dense(10)\n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense_2(x)\n",
    "        return x\n",
    "\n",
    "input_data = tf.random.uniform([60, 28, 28])\n",
    "\n",
    "eager_model = SequentialModel()\n",
    "graph_model = tf.function(eager_model)\n",
    "\n",
    "print(\"Eager time:\", timeit.timeit(lambda: eager_model(input_data), number=10000))\n",
    "print(\"Graph time:\", timeit.timeit(lambda: graph_model(input_data), number=10000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual line\n",
    "TRUE_W = 3.0\n",
    "TRUE_B = 2.0\n",
    "\n",
    "NUM_EXAMPLES = 1000\n",
    "\n",
    "# A vector of random x values\n",
    "x = tf.random.normal(shape=[NUM_EXAMPLES])\n",
    "\n",
    "# Generate some noise\n",
    "noise = tf.random.normal(shape=[NUM_EXAMPLES])\n",
    "\n",
    "# Calculate y\n",
    "y = x * TRUE_W + TRUE_B + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a linear regression model and loss function\n",
    "# This computes a single loss value for an entire batch\n",
    "def loss(target_y, predicted_y):\n",
    "    return tf.reduce_mean(tf.square(target_y - predicted_y))\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Initialize the weights to `5.0` and the bias to `0.0`\n",
    "        # In practice, these should be randomly initialized\n",
    "        self.w = tf.Variable(5.0)\n",
    "        self.b = tf.Variable(0.0)\n",
    "    def __call__(self, x, **kwargs):\n",
    "        return self.w * x + self.b\n",
    "\n",
    "model = MyModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0: W=4.58 b=0.40, loss=9.21292\n",
      "Epoch  1: W=4.24 b=0.72, loss=6.16160\n",
      "Epoch  2: W=3.98 b=0.98, loss=4.24380\n",
      "Epoch  3: W=3.77 b=1.18, loss=3.03835\n",
      "Epoch  4: W=3.61 b=1.34, loss=2.28061\n",
      "Epoch  5: W=3.48 b=1.47, loss=1.80427\n",
      "Epoch  6: W=3.38 b=1.57, loss=1.50479\n",
      "Epoch  7: W=3.30 b=1.66, loss=1.31651\n",
      "Epoch  8: W=3.23 b=1.72, loss=1.19811\n",
      "Epoch  9: W=3.18 b=1.77, loss=1.12367\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "learning_rate=0.1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Update the model with the single giant batch\n",
    "    with tf.GradientTape() as t:\n",
    "        # Trainable variables are automatically tracked by GradientTape\n",
    "        current_loss = loss(y, model(x))\n",
    "    # Use GradientTape to calculate the gradients with respect to W and b\n",
    "    dw, db = t.gradient(current_loss, [model.w, model.b])\n",
    "    \n",
    "    # Subtract the gradient scaled by the learning rate\n",
    "    model.w.assign_sub(learning_rate * dw)\n",
    "    model.b.assign_sub(learning_rate * db)\n",
    "\n",
    "    print(\"Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f\" %\n",
    "          (epoch, model.w.numpy(), model.b.numpy(), current_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.2129\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 998us/step - loss: 6.1616\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 995us/step - loss: 4.2438\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.0384\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 995us/step - loss: 2.2806\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 971us/step - loss: 1.8043\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.5048\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3165\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 995us/step - loss: 1.1981\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1aac0030288>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_model = MyModel()\n",
    "\n",
    "keras_model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
    "    loss=tf.keras.losses.mean_squared_error,\n",
    ")\n",
    "\n",
    "keras_model.fit(x, y, epochs=10, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
