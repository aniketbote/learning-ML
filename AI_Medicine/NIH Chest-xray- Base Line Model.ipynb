{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "# import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "\n",
    "%run \"Custom Loss and Custom Metric.ipynb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "train_df = pd.read_csv('../data/archive/train_df.csv')\n",
    "# test_df = pd.read_csv('../data/archive/test_df.csv')\n",
    "val_df = pd.read_csv('../data/archive/val_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 74833 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "# Generate Ramdon sample for somputing statitics\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "xcol = 'path'\n",
    "ycol = list(train_df.columns[3:18])\n",
    "random_sample_generator = datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col = 'path',\n",
    "    y_col = ycol,\n",
    "    target_size=(300, 300),\n",
    "    batch_size=100,\n",
    "    class_mode='raw',\n",
    "    seed = 1000\n",
    ")\n",
    "random_sample_data = next(random_sample_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.preprocessing.normalization.Normalization at 0x23a00b3ee88>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computing statistics for normalize layer \n",
    "normalize_layer = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "normalize_layer.adapt(random_sample_data[0])\n",
    "del random_sample_data\n",
    "normalize_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "IMAGE_SIZE = (224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 74833 validated image filenames.\n",
      "Found 11504 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "# Creating data generators\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "xcol = 'path'\n",
    "ycol = list(train_df.columns[3:18])\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col = 'path',\n",
    "    y_col = ycol,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='raw',\n",
    "    seed = 100\n",
    ")\n",
    "\n",
    "# test_generator = datagen.flow_from_dataframe(\n",
    "#     test_df,\n",
    "#     x_col = 'path',\n",
    "#     target_size=IMAGE_SIZE,\n",
    "#     batch_size=1,\n",
    "#     class_mode=None,\n",
    "#     seed = 200\n",
    "# )\n",
    "\n",
    "val_generator = datagen.flow_from_dataframe(\n",
    "    val_df,\n",
    "    x_col = 'path',\n",
    "    y_col = ycol,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='raw',\n",
    "    seed = 300\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Getting Sample data for display \n",
    "# sample_data = next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessing data\n",
    "# rezize_data = tf.keras.layers.experimental.preprocessing.Resizing(IMAGE_SIZE[0],IMAGE_SIZE[1])(sample_data[0])\n",
    "# rotated_data = tf.keras.layers.experimental.preprocessing.RandomRotation((-0.04,0.05))(rezize_data)\n",
    "# rescale_data = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(rotated_data)\n",
    "# normalized_data = normalize_layer(rotated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ploting the preprocessed data\n",
    "# plt.figure(figsize=(20, 20))\n",
    "# for i in range(BATCH_SIZE):\n",
    "#     ax = plt.subplot(10, 4, i + 1)\n",
    "#     plt.imshow(np.clip(rescale_data[i],0, 1))\n",
    "#     plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ploting the unpreprocessed data\n",
    "# plt.figure(figsize=(20, 20))\n",
    "# for i in range(BATCH_SIZE):\n",
    "#     ax = plt.subplot(8, 4, i + 1)\n",
    "#     plt.imshow(sample_data[0][i].astype('uint8'))\n",
    "#     plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cardiomegaly           1411\n",
       "Emphysema              1337\n",
       "Effusion               8161\n",
       "No Finding            42358\n",
       "Hernia                  102\n",
       "Infiltration          12350\n",
       "Mass                   3654\n",
       "Nodule                 4192\n",
       "Atelectasis            7606\n",
       "Pneumothorax           2571\n",
       "Pleural_Thickening     1990\n",
       "Pneumonia               814\n",
       "Fibrosis               1028\n",
       "Edema                  1339\n",
       "Consolidation          2680\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating positive weights and negative weights\n",
    "n_pos = train_df.iloc[:,3:18].sum(axis = 0).values\n",
    "n_neg = train_df.shape[0] - n_pos\n",
    "# wp = n_neg / train_df.shape[0]\n",
    "# wn = n_pos / train_df.shape[0]\n",
    "wp = (1 / n_pos) * (train_df.shape[0]) / 2\n",
    "wn = (1 / n_neg) * (train_df.shape[0]) / 2\n",
    "train_df.iloc[:,3:18].sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 26.51771793,  27.98541511,   4.58479353,   0.88333963,\n",
       "       366.82843137,   3.02967611,  10.23987411,   8.92569179,\n",
       "         4.91933999,  14.55328666,  18.80226131,  45.96621622,\n",
       "        36.39737354,  27.94361464,  13.9613806 ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50960884, 0.50909573, 0.5612026 , 1.1521632 , 0.50068245,\n",
       "       0.59882688, 0.52566768, 0.52967115, 0.55656953, 0.51778943,\n",
       "       0.51365951, 0.50549859, 0.5069643 , 0.50910959, 0.51857165])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Model\n",
    "def create_model(pretrained_model, wp, wn):\n",
    "    inputs = tf.keras.Input(shape = (IMAGE_SIZE[0],IMAGE_SIZE[1],3))\n",
    "    x = tf.keras.layers.experimental.preprocessing.Resizing(IMAGE_SIZE[0], IMAGE_SIZE[1])(inputs)\n",
    "    x = tf.keras.layers.experimental.preprocessing.RandomRotation((-0.04,0.05))(x)\n",
    "    x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(x)\n",
    "    x = pretrained_model(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Dense(128, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    outputs = tf.keras.layers.Dense(15, activation = 'sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "              loss= WeightedBinaryCrossEntropy(wp = wp, wn = wn),\n",
    "              metrics=[tfa.metrics.F1Score(num_classes = 15, average = 'macro', threshold = 0.5,name = 'f1_macro'), MTCF1Score(15), tfa.metrics.F1Score(num_classes = 15, average = 'micro', threshold = 0.5,name = 'f1_micro')])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "vgg16 = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_shape = (IMAGE_SIZE[0],IMAGE_SIZE[1],3))\n",
    "# vgg16.trainable = False\n",
    "model = create_model(vgg16, wp, wn)\n",
    "# tf.keras.utils.plot_model(model, 'my_first_model.png', show_shapes=True)\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating callbacks\n",
    "weight_path=\"{}_weights_best.h5\".format('VGG')\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min')\n",
    "\n",
    "early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=3)\n",
    "\n",
    "callbacks_list = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   2/4678 [..............................] - ETA: 20:44 - loss: 29.7945 - f1_macro: 0.1182 - mtc_f1_score: 0.0000e+00 - f1_micro: 0.1875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1555s vs `on_train_batch_end` time: 0.3729s). Check your callbacks.\n",
      "4678/4678 [==============================] - ETA: 0s - loss: 10.6454 - f1_macro: 0.1072 - mtc_f1_score: 0.0237 - f1_micro: 0.1176\n",
      "Epoch 00001: val_loss improved from inf to 9.35231, saving model to VGG_weights_best.h5\n",
      "4678/4678 [==============================] - 3226s 690ms/step - loss: 10.6454 - f1_macro: 0.1072 - mtc_f1_score: 0.0237 - f1_micro: 0.1176 - val_loss: 9.3523 - val_f1_macro: 0.0388 - val_mtc_f1_score: 0.0271 - val_f1_micro: 0.0513\n",
      "Epoch 2/10\n",
      "4678/4678 [==============================] - ETA: 0s - loss: 10.6543 - f1_macro: 0.1074 - mtc_f1_score: 0.0257 - f1_micro: 0.1136\n",
      "Epoch 00002: val_loss improved from 9.35231 to 9.31544, saving model to VGG_weights_best.h5\n",
      "4678/4678 [==============================] - 2811s 601ms/step - loss: 10.6543 - f1_macro: 0.1074 - mtc_f1_score: 0.0257 - f1_micro: 0.1136 - val_loss: 9.3154 - val_f1_macro: 0.0764 - val_mtc_f1_score: 0.0268 - val_f1_micro: 0.1427\n",
      "Epoch 3/10\n",
      "4678/4678 [==============================] - ETA: 0s - loss: 10.6235 - f1_macro: 0.1106 - mtc_f1_score: 0.0264 - f1_micro: 0.1260\n",
      "Epoch 00003: val_loss did not improve from 9.31544\n",
      "4678/4678 [==============================] - 3981s 851ms/step - loss: 10.6235 - f1_macro: 0.1106 - mtc_f1_score: 0.0264 - f1_micro: 0.1260 - val_loss: 9.8484 - val_f1_macro: 0.0764 - val_mtc_f1_score: 0.0263 - val_f1_micro: 0.2864\n",
      "Epoch 4/10\n",
      "4678/4678 [==============================] - ETA: 0s - loss: 10.4787 - f1_macro: 0.1085 - mtc_f1_score: 0.0256 - f1_micro: 0.1202\n",
      "Epoch 00004: val_loss improved from 9.31544 to 9.30376, saving model to VGG_weights_best.h5\n",
      "4678/4678 [==============================] - 31120s 7s/step - loss: 10.4787 - f1_macro: 0.1085 - mtc_f1_score: 0.0256 - f1_micro: 0.1202 - val_loss: 9.3038 - val_f1_macro: 0.0811 - val_mtc_f1_score: 0.0262 - val_f1_micro: 0.1375\n",
      "Epoch 5/10\n",
      "4678/4678 [==============================] - ETA: 0s - loss: 10.4756 - f1_macro: 0.1135 - mtc_f1_score: 0.0259 - f1_micro: 0.1499\n",
      "Epoch 00005: val_loss improved from 9.30376 to 9.29907, saving model to VGG_weights_best.h5\n",
      "4678/4678 [==============================] - 2584s 552ms/step - loss: 10.4756 - f1_macro: 0.1135 - mtc_f1_score: 0.0259 - f1_micro: 0.1499 - val_loss: 9.2991 - val_f1_macro: 0.0911 - val_mtc_f1_score: 0.0263 - val_f1_micro: 0.1506\n",
      "Epoch 6/10\n",
      "4678/4678 [==============================] - ETA: 0s - loss: 10.4660 - f1_macro: 0.1104 - mtc_f1_score: 0.0259 - f1_micro: 0.1223\n",
      "Epoch 00006: val_loss did not improve from 9.29907\n",
      "4678/4678 [==============================] - 2895s 619ms/step - loss: 10.4660 - f1_macro: 0.1104 - mtc_f1_score: 0.0259 - f1_micro: 0.1223 - val_loss: 9.3416 - val_f1_macro: 0.0497 - val_mtc_f1_score: 0.0263 - val_f1_micro: 0.0604\n",
      "Epoch 7/10\n",
      " 978/4678 [=====>........................] - ETA: 46:47 - loss: 10.3596 - f1_macro: 0.0913 - mtc_f1_score: 0.0262 - f1_micro: 0.0993"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-d103063ff594>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m ) \n",
      "\u001b[1;32mc:\\users\\aniket\\documents\\aniket\\learning-ml\\ml_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aniket\\documents\\aniket\\learning-ml\\ml_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aniket\\documents\\aniket\\learning-ml\\ml_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aniket\\documents\\aniket\\learning-ml\\ml_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aniket\\documents\\aniket\\learning-ml\\ml_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aniket\\documents\\aniket\\learning-ml\\ml_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aniket\\documents\\aniket\\learning-ml\\ml_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aniket\\documents\\aniket\\learning-ml\\ml_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\aniket\\documents\\aniket\\learning-ml\\ml_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training model\n",
    "model.fit(\n",
    "    train_generator, \n",
    "    epochs=10, \n",
    "    steps_per_epoch = np.ceil(train_generator.n/BATCH_SIZE), \n",
    "    validation_data = val_generator,\n",
    "    validation_steps = np.ceil(val_generator.n/BATCH_SIZE),\n",
    "    callbacks = callbacks_list\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Model\n",
    "saved_model = tf.keras.models.load_model('xray_class_weights_best.h5')\n",
    "saved_model.evaluate(\n",
    "    val_generator,\n",
    "    steps = np.ceil(val_generator.n/BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Predictions\n",
    "y_preds = saved_model.predict(\n",
    "    val_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = val_df.iloc[:,3:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (9, 9))\n",
    "for (idx, c_label) in enumerate(ycol):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true[:,idx].astype(int), y_preds[:,idx])\n",
    "    c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
    "c_ax.legend()\n",
    "c_ax.set_xlabel('False Positive Rate')\n",
    "c_ax.set_ylabel('True Positive Rate')\n",
    "fig.savefig('barely_trained_net.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 1\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# loss_fn = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epochs in range(EPOCHS):\n",
    "#     for step, (X_batch_train, y_batch_train) in enumerate(train_generator):\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             y_preds = model(X_batch_train, training = True)\n",
    "#             loss_val = loss_fn(y_batch_train, y_preds)\n",
    "#         gradients = tape.gradient(loss_val, model.trainable_weights)\n",
    "#         optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "#         if step % 200 == 0:\n",
    "#             t0\n",
    "#         print(\"Training loss (for one batch) at step %d: %.4f\"% (step, float(loss_val)), end = '\\r')\n",
    "#         if step == np.ceil(train_generator.n/BATCH_SIZE):\n",
    "#             break\n",
    "#     print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
